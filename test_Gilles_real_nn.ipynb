{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ccbc3caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm         # make loops show as a smart progress meter\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bc9be5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Classic_Model (ReLu,  bias, not orthogonal, Softmax)========================================================================================================\n",
    "class ClassicalModel_Relu_Bias(nn.Module):\n",
    "    def __init__(self, hidden_sizes, input_size=4, output_size=3):\n",
    "        \"\"\"\n",
    "        input_size: number of input features (e.g. 4 for Iris)\n",
    "        hidden_sizes: list of hidden layer sizes, e.g. [8, 8, 4]\n",
    "        output_size: number of output classes (e.g. 3 for Iris)\n",
    "        \"\"\"\n",
    "        super(ClassicalModel_Relu_Bias, self).__init__()\n",
    "        \n",
    "        # Create a list of layer sizes\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Create a list of Linear layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            for i in range(len(layer_sizes) - 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Go through all layers except the last one with ReLU\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        # Apply softmax on the last layer (output)\n",
    "        x = F.softmax(self.layers[-1](x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c54746af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Classic_Model_NoBias (ReLU, no bias, Not orthogonal, SoftMax)============================================================================================\n",
    "class ClassicalModel_ReLU_NoBias(nn.Module):\n",
    "    def __init__(self, hidden_sizes,input_size=4, output_size=3):\n",
    "        \"\"\"\n",
    "        ReLU, no bias, standard weights, SoftMax\n",
    "        \"\"\"\n",
    "        super(ClassicalModel_ReLU_NoBias, self).__init__()\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layer_sizes[i], layer_sizes[i+1], bias=False)\n",
    "            for i in range(len(layer_sizes)-1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = F.softmax(self.layers[-1](x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6a8fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Classic_Model_NoRelu (No ReLu, bias, Not orthogonal, SoftMax)========================================================================================================\n",
    "class ClassicalModel_NoRelu_Bias(nn.Module):\n",
    "    def __init__(self, hidden_sizes, input_size=4, output_size=3):\n",
    "        \"\"\"\n",
    "        input_size: number of input features (e.g. 4 for Iris)\n",
    "        hidden_sizes: list of hidden layer sizes, e.g. [8, 8, 4]\n",
    "        output_size: number of output classes (e.g. 3 for Iris)\n",
    "        \"\"\"\n",
    "        super(ClassicalModel_NoRelu_Bias, self).__init__()\n",
    "        \n",
    "        # Create a list of layer sizes\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Create a list of Linear layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layer_sizes[i], layer_sizes[i+1], bias=False)\n",
    "            for i in range(len(layer_sizes)-1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x): #defines how an input x becomes an output, passing through the networkâ€™s layers.\n",
    "        # Go through all layers except the last one with ReLU\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "        # Apply softmax on the last layer (output)\n",
    "        x = F.softmax(self.layers[-1](x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ea40673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Classic_Model_NoReLU_NoBias (No ReLU, no bias, Not orthogonal, SoftMax)===============================================================================\n",
    "class ClassicalModel_NoReLU_NoBias(nn.Module):\n",
    "    def __init__(self, hidden_sizes, input_size=4, output_size=3):\n",
    "        \"\"\"\n",
    "        input_size: number of input features (e.g. 4 for Iris)\n",
    "        hidden_sizes: list of hidden layer sizes, e.g. [8, 8, 4]\n",
    "        output_size: number of output classes (e.g. 3 for Iris)\n",
    "        \"\"\"\n",
    "        super(ClassicalModel_NoReLU_NoBias, self).__init__()\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layer_sizes[i], layer_sizes[i+1], bias=False)\n",
    "            for i in range(len(layer_sizes)-1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "        x = F.softmax(self.layers[-1](x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0d9a10d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Classic_Model_Ortho_ReLU_Bias (ReLU, bias, orthogonal, SoftMax)======================================================================================\n",
    "class ClassicalModel_Ortho_ReLU_Bias(nn.Module):\n",
    "    def __init__(self, hidden_sizes,input_size=4, output_size=3):\n",
    "        \"\"\"\n",
    "        ReLU, bias, orthogonal weights, SoftMax\n",
    "        \"\"\"\n",
    "        super(ClassicalModel_Ortho_ReLU_Bias, self).__init__()\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.utils.parametrizations.orthogonal(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            for i in range(len(layer_sizes)-1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = F.softmax(self.layers[-1](x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "44ed84aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Classic_Model_Ortho_ReLU_NoBias (ReLU, no bias, orthogonal, SoftMax)================================================================================\n",
    "class ClassicalModel_Ortho_ReLU_NoBias(nn.Module):\n",
    "    def __init__(self, hidden_sizes, input_size=4, output_size=3):\n",
    "        \"\"\"\n",
    "        ReLU, no bias, orthogonal weights, SoftMax\n",
    "        \"\"\"\n",
    "        super(ClassicalModel_Ortho_ReLU_NoBias, self).__init__()\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.utils.parametrizations.orthogonal(nn.Linear(layer_sizes[i], layer_sizes[i+1], bias=False))\n",
    "            for i in range(len(layer_sizes)-1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        x = F.softmax(self.layers[-1](x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "806a65ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Classic_Model_Ortho_NoReLU_Bias (No ReLU, bias, orthogonal, SoftMax)===============================================================================\n",
    "class ClassicalModel_Ortho_NoReLU_Bias(nn.Module):\n",
    "    def __init__(self, hidden_sizes, input_size=4, output_size=3):\n",
    "        \"\"\"\n",
    "        No ReLU, bias, orthogonal weights, SoftMax\n",
    "        \"\"\"\n",
    "        super(ClassicalModel_Ortho_NoReLU_Bias, self).__init__()\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.utils.parametrizations.orthogonal(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "            for i in range(len(layer_sizes)-1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "        x = F.softmax(self.layers[-1](x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3a28a675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 8) Classic_Model_Ortho_NoReLU_NoBias (No ReLU, no bias, orthogonal, SoftMax)=========================================================================\n",
    "class ClassicalModel_Ortho_NoReLU_NoBias(nn.Module):\n",
    "    def __init__(self, hidden_sizes,input_size=4, output_size=3):\n",
    "        \"\"\"\n",
    "        No ReLU, no bias, orthogonal weights, SoftMax\n",
    "        \"\"\"\n",
    "        super(ClassicalModel_Ortho_NoReLU_NoBias, self).__init__()\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.utils.parametrizations.orthogonal(nn.Linear(layer_sizes[i], layer_sizes[i+1], bias=False))\n",
    "            for i in range(len(layer_sizes)-1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "        x = F.softmax(self.layers[-1](x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04f7be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnitaryLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Complex square unitary layer using matrix exponential.\n",
    "    Models an ideal MZI mesh.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, width):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "\n",
    "        # Trainable complex matrix (skew-Hermitian enforced in forward)\n",
    "        self.A = nn.Parameter(torch.randn(width, width, dtype=torch.complex64) * 1e-3)\n",
    "\n",
    "    def forward(self, E):\n",
    "        \"\"\"\n",
    "        E: complex electric field, shape (batch, width)\n",
    "        Returns:\n",
    "            E_out: complex field after unitary transformation\n",
    "        \"\"\"\n",
    "        # Enforce skew-Hermitian: Aâ€  = -A\n",
    "        A_skew = self.A - self.A.conj().T\n",
    "\n",
    "        # Exponentiate â†’ exact unitary\n",
    "        U = torch.matrix_exp(A_skew)  # shape (width, width)\n",
    "\n",
    "        # Multiply input field by unitary\n",
    "        # batch @ width times width â†’ (batch, width)\n",
    "        return E @ U.T\n",
    "\n",
    "class PhotonicNNMatrixExp(nn.Module):\n",
    "    def __init__(self, width, depth, input_size, output_size, nonlinearity=\"relu\"):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "        self.nonlinearity = nonlinearity\n",
    "        self.meshes = nn.ModuleList([UnitaryLayer(width) for _ in range(depth)])\n",
    "        self.classifier = nn.Linear(width, output_size, bias=True)\n",
    "\n",
    "    # --- Methods for photonic operations ---\n",
    "    def photodetect(self, E):\n",
    "        return torch.real(E * E.conj())\n",
    "\n",
    "    def eo_nonlinearity(self, I):\n",
    "        if self.nonlinearity == \"relu\":\n",
    "            return torch.relu(I)\n",
    "        elif self.nonlinearity == \"tanh\":\n",
    "            return torch.tanh(I)\n",
    "        elif self.nonlinearity == \"none\":\n",
    "            return I\n",
    "        else:\n",
    "            raise ValueError(\"Unknown nonlinearity\")\n",
    "\n",
    "    def intensity_to_field(self, I):\n",
    "        return torch.sqrt(torch.clamp(I, min=1e-12)).to(torch.complex64)\n",
    "\n",
    "    def forward(self, I_input):\n",
    "        batch_size = I_input.shape[0]\n",
    "        I = torch.zeros(batch_size, self.width, device=I_input.device)\n",
    "        I[:, :I_input.shape[1]] = I_input\n",
    "\n",
    "        E = self.intensity_to_field(I)\n",
    "\n",
    "        for mesh in self.meshes:\n",
    "            E = mesh(E)\n",
    "            I = self.photodetect(E)\n",
    "            I = self.eo_nonlinearity(I)\n",
    "            E = self.intensity_to_field(I)\n",
    "\n",
    "        I_out = self.photodetect(E)\n",
    "        return torch.softmax(self.classifier(I_out), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9ac76a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_backprop():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        iris = load_iris()\n",
    "        X = iris['data']\n",
    "        y = torch.tensor(iris['target'], dtype=torch.long)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = torch.tensor(scaler.fit_transform(X), dtype=torch.float)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=0.2, random_state=2)  # Total n dat 150, so 120 for train and 30 for test\n",
    "\n",
    "        self.train_data = TensorDataset(X_train, y_train)\n",
    "        test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_data, shuffle=True, batch_size=12)  # batch size 12 so 10 batch\n",
    "        self.test_loader = DataLoader(test_data, batch_size=len(test_data.tensors[0]))  # batch size 30 max\n",
    "\n",
    "    def train_IRIS(self, model, epochs: int = 400, epsilon: float = 0.02,\n",
    "               log_first_layer=False, max_snapshots=10):\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_loss = np.zeros(epochs)\n",
    "        valid_loss = np.zeros(epochs)\n",
    "        train_accuracy = np.zeros(epochs)\n",
    "        valid_accuracy = np.zeros(epochs)\n",
    "\n",
    "        # ðŸ”¹ storage for matrices\n",
    "        first_layer_matrices = []\n",
    "\n",
    "        snapshot_count = 0\n",
    "\n",
    "        for epoch in tqdm.trange(epochs):\n",
    "            for X, y in self.train_loader:\n",
    "                y_pred = model(X)\n",
    "                loss = loss_fn(y_pred, y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                # ðŸ”¹ log first-layer matrix\n",
    "                if log_first_layer and snapshot_count < max_snapshots:\n",
    "                    W = model.layers[0].weight.detach().cpu().numpy()\n",
    "                    first_layer_matrices.append(W.copy())\n",
    "                    snapshot_count += 1\n",
    "\n",
    "            # --- metrics (unchanged) ---\n",
    "            with torch.no_grad():\n",
    "                X_train, y_train = self.train_data[:]\n",
    "                y_pred_train = model(X_train)\n",
    "                train_loss[epoch] = loss_fn(y_pred_train, y_train).item()\n",
    "                y_pred_labels = torch.argmax(y_pred_train, axis=1)\n",
    "                train_accuracy[epoch] = torch.mean((y_pred_labels == y_train).float()).item()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                X_val, y_val = next(iter(self.test_loader))\n",
    "                y_pred_val = model(X_val)\n",
    "                valid_loss[epoch] = loss_fn(y_pred_val, y_val).item()\n",
    "                y_pred_labels = torch.argmax(y_pred_val, axis=1)\n",
    "                valid_accuracy[epoch] = torch.mean((y_pred_labels == y_val).float()).item()\n",
    "\n",
    "        final_train_loss = train_loss[-1]\n",
    "        final_valid_loss = valid_loss[-1]\n",
    "\n",
    "        epochs_to_final_train = np.argmax(train_loss <= final_train_loss * (1 + epsilon)) + 1\n",
    "        epochs_to_final_valid = np.argmax(valid_loss <= final_valid_loss * (1 + epsilon)) + 1\n",
    "\n",
    "        return (train_loss, valid_loss, train_accuracy, valid_accuracy,\n",
    "                epochs_to_final_train, epochs_to_final_valid,\n",
    "                first_layer_matrices)\n",
    "\n",
    "        \n",
    "\n",
    "    def graph_train_IRIS(self, model, epochs: int = 400, n_runs: int = 15, epsilon: float = 0.02) -> None:\n",
    "        \"\"\"\n",
    "        Train the given model multiple times (n_runs) and save all results in a single CSV file.\n",
    "        For the FIRST run only, print the first 10 first-layer weight matrices\n",
    "        (after optimizer steps) to verify orthogonality.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        from datetime import datetime\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "        # Detect hidden layer sizes\n",
    "        try:\n",
    "            hidden_sizes = [layer.out_features for layer in model.layers[:-1]]\n",
    "        except AttributeError:\n",
    "            hidden_sizes = [\"Unknown\"]\n",
    "\n",
    "        global_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        all_runs = []\n",
    "\n",
    "        for run in range(1, n_runs + 1):\n",
    "            # âš¡ pass hidden_sizes explicitly\n",
    "            model_instance = copy.deepcopy(model)\n",
    "            print(f\"\\nðŸš€ Run {run}/{n_runs} for {model_name} with layers {hidden_sizes}\")\n",
    "\n",
    "            optimizer = torch.optim.Adam(model_instance.parameters(), lr=0.001)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "            train_loss = np.zeros(epochs)\n",
    "            valid_loss = np.zeros(epochs)\n",
    "            train_accuracy = np.zeros(epochs)\n",
    "            valid_accuracy = np.zeros(epochs)\n",
    "\n",
    "            printed = 0  # number of matrices printed so far\n",
    "\n",
    "            for epoch in range(epochs):\n",
    "                # --- TRAIN ---\n",
    "                for X, y in self.train_loader:\n",
    "                    y_pred = model_instance(X)\n",
    "                    loss = loss_fn(y_pred, y)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # ðŸ”¹ PRINT FIRST 10 MATRICES (only run 1)\n",
    "                    if run == 1 and printed < 10:\n",
    "                        try:\n",
    "                            # Classical NN\n",
    "                            W = model_instance.layers[0].weight.detach().cpu().numpy()\n",
    "                        except AttributeError:\n",
    "                            # Photonic NN\n",
    "                            W = model_instance.meshes[0].A.detach().cpu().numpy()\n",
    "                        print(f\"\\n--- First-layer weight matrix (step {printed + 1}) ---\")\n",
    "                        print(W)\n",
    "\n",
    "                        # Orthogonality check\n",
    "                        try:\n",
    "                            # Classical\n",
    "                            if W.shape[0] <= W.shape[1]:\n",
    "                                M = W @ W.T\n",
    "                                label = \"W Wáµ€\"\n",
    "                            else:\n",
    "                                M = W.T @ W\n",
    "                                label = \"Wáµ€ W\"\n",
    "                        except Exception:\n",
    "                            # Photonic: A is complex, check unitarity via matrix exponential\n",
    "                            A_skew = model_instance.meshes[0].A - model_instance.meshes[0].A.conj().T\n",
    "                            U = torch.matrix_exp(A_skew).detach().cpu().numpy()\n",
    "                            M = U.conj().T @ U\n",
    "                            label = \"Uâ€  U (should be identity)\"\n",
    "\n",
    "                        print(f\"{label} (should be close to identity):\")\n",
    "                        print(M)\n",
    "\n",
    "                        printed += 1\n",
    "\n",
    "                # --- TRAIN METRICS ---\n",
    "                with torch.no_grad():\n",
    "                    X_train, y_train = self.train_data[:]\n",
    "                    y_pred_train = model_instance(X_train)\n",
    "                    train_loss[epoch] = loss_fn(y_pred_train, y_train).item()\n",
    "                    train_accuracy[epoch] = (\n",
    "                        torch.argmax(y_pred_train, dim=1) == y_train\n",
    "                    ).float().mean().item()\n",
    "\n",
    "                # --- VALIDATION METRICS ---\n",
    "                with torch.no_grad():\n",
    "                    X_val, y_val = next(iter(self.test_loader))\n",
    "                    y_pred_val = model_instance(X_val)\n",
    "                    valid_loss[epoch] = loss_fn(y_pred_val, y_val).item()\n",
    "                    valid_accuracy[epoch] = (\n",
    "                        torch.argmax(y_pred_val, dim=1) == y_val\n",
    "                    ).float().mean().item()\n",
    "\n",
    "            # --- FINAL METRICS ---\n",
    "            final_train_loss = train_loss[-1]\n",
    "            final_valid_loss = valid_loss[-1]\n",
    "            final_train_acc = train_accuracy[-1]\n",
    "            final_valid_acc = valid_accuracy[-1]\n",
    "\n",
    "            epochs_to_final_train = np.argmax(train_loss <= final_train_loss * (1 + epsilon)) + 1\n",
    "            epochs_to_final_valid = np.argmax(valid_loss <= final_valid_loss * (1 + epsilon)) + 1\n",
    "\n",
    "            all_runs.append({\n",
    "                \"timestamp\": global_timestamp,\n",
    "                \"model_name\": model_name,\n",
    "                \"hidden_sizes\": str(hidden_sizes),\n",
    "                \"epochs\": epochs,\n",
    "                \"run_number\": run,\n",
    "                \"final_train_loss\": final_train_loss,\n",
    "                \"final_valid_loss\": final_valid_loss,\n",
    "                \"final_train_accuracy\": final_train_acc,\n",
    "                \"final_valid_accuracy\": final_valid_acc,\n",
    "                \"epochs_to_final_train\": epochs_to_final_train,\n",
    "                \"epochs_to_final_valid\": epochs_to_final_valid\n",
    "            })\n",
    "\n",
    "            print(f\"ðŸ Validation accuracy (run {run}): {final_valid_acc:.3f}\")\n",
    "            print(f\"â±ï¸ Epochs to converge (train/valid): {epochs_to_final_train}/{epochs_to_final_valid}\")\n",
    "\n",
    "            # --- PLOT ONLY FOR LAST RUN ---\n",
    "            if run == n_runs:\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "                ax1.plot(train_accuracy, label=\"Train accuracy\")\n",
    "                ax1.plot(valid_accuracy, label=\"Validation accuracy\")\n",
    "                ax1.set_title(\"Accuracy\")\n",
    "                ax1.set_xlabel(\"Epoch\")\n",
    "                ax1.set_ylim([0.0, 1.05])\n",
    "                ax1.legend()\n",
    "\n",
    "                ax2.plot(train_loss, label=\"Train loss\")\n",
    "                ax2.plot(valid_loss, label=\"Validation loss\")\n",
    "                ax2.set_title(\"Loss\")\n",
    "                ax2.set_xlabel(\"Epoch\")\n",
    "                ax2.legend()\n",
    "\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "        # --- SAVE RESULTS ---\n",
    "        results_df = pd.DataFrame(all_runs)\n",
    "        csv_filename = f\"{model_name}_layers{hidden_sizes}_epochs{epochs}_{n_runs}runs_{global_timestamp}.csv\"\n",
    "        csv_path = os.path.join(\"results\", csv_filename)\n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "        print(f\"\\nâœ… All {n_runs} runs saved in one file â†’ {csv_path}\")\n",
    "    # ======================================================    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4eeceb70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Run 1/15 for PhotonicNNMatrixExp with layers ['Unknown']\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "PhotonicNNMatrixExp.intensity_to_field() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[51]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m depth \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m]:\n\u001b[32m      5\u001b[39m     model = PhotonicNNMatrixExp(width=width,depth=depth, input_size=\u001b[32m4\u001b[39m, output_size=\u001b[32m3\u001b[39m, nonlinearity=\u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgraph_train_IRIS\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 120\u001b[39m, in \u001b[36mtrain_backprop.graph_train_IRIS\u001b[39m\u001b[34m(self, model, epochs, n_runs, epsilon)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m    118\u001b[39m     \u001b[38;5;66;03m# --- TRAIN ---\u001b[39;00m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.train_loader:\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         y_pred = \u001b[43mmodel_instance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    121\u001b[39m         loss = loss_fn(y_pred, y)\n\u001b[32m    123\u001b[39m         optimizer.zero_grad()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gilles\\Documents\\school\\Ma2\\Thesis\\VSC\\photonics_nn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Gilles\\Documents\\school\\Ma2\\Thesis\\VSC\\photonics_nn\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[49]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mPhotonicNNMatrixExp.forward\u001b[39m\u001b[34m(self, I_input)\u001b[39m\n\u001b[32m     58\u001b[39m I = torch.zeros(batch_size, \u001b[38;5;28mself\u001b[39m.width, device=I_input.device)\n\u001b[32m     59\u001b[39m I[:, :I_input.shape[\u001b[32m1\u001b[39m]] = I_input\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m E = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mintensity_to_field\u001b[49m\u001b[43m(\u001b[49m\u001b[43mI\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mesh \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.meshes:\n\u001b[32m     64\u001b[39m     E = mesh(E)\n",
      "\u001b[31mTypeError\u001b[39m: PhotonicNNMatrixExp.intensity_to_field() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "a = train_backprop()\n",
    "\n",
    "for width in [4, 5]:\n",
    "    for depth in [1, 2]:\n",
    "        model = PhotonicNNMatrixExp(width=width,depth=depth, input_size=4, output_size=3, nonlinearity=\"relu\")\n",
    "        a.graph_train_IRIS(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a345147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0adf09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "photonics_nn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
