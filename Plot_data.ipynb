{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbec662c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob, os\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import tqdm         # make loops show as a smart progress meter\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2390693c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic_Model (ReLu and bias)========================================================================================================\n",
    "# Variable is added to the name because layers and neurons can now be chosen\n",
    "class ClassicalVariableModel(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_sizes=[8, 8], output_size=3):\n",
    "        \"\"\"\n",
    "        input_size: number of input features (e.g. 4 for Iris)\n",
    "        hidden_sizes: list of hidden layer sizes, e.g. [8, 8, 4]\n",
    "        output_size: number of output classes (e.g. 3 for Iris)\n",
    "        \"\"\"\n",
    "        super(ClassicalVariableModel, self).__init__()\n",
    "        \n",
    "        # Create a list of layer sizes\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Create a list of Linear layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            for i in range(len(layer_sizes) - 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Go through all layers except the last one with ReLU\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        # Apply softmax on the last layer (output)\n",
    "        x = F.softmax(self.layers[-1](x), dim=1)\n",
    "        return x\n",
    "    \n",
    "# Classic_Model_No_Biases (Relu, No bias) ========================================================================================================\n",
    "class ClassicalVariableModel_NoBiases(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_sizes=[8, 8], output_size=3):\n",
    "        \"\"\"\n",
    "        input_size: number of input features (e.g. 4 for Iris)\n",
    "        hidden_sizes: list of hidden layer sizes, e.g. [8, 8, 4]\n",
    "        output_size: number of output classes (e.g. 3 for Iris)\n",
    "        \"\"\"\n",
    "        super(ClassicalVariableModel_NoBiases, self).__init__()\n",
    "        \n",
    "        # Create a list of layer sizes\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Create a list of Linear layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layer_sizes[i], layer_sizes[i+1], bias=False)\n",
    "            for i in range(len(layer_sizes) - 1)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Go through all layers except the last one with ReLU\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        # Apply softmax on the last layer (output)\n",
    "        x = F.softmax(self.layers[-1](x), dim=1)\n",
    "        return x\n",
    "    \n",
    "# Classic_Model_Relu (ReLu, bias)========================================================================================================\n",
    "class ClassicalVariableModel_Relu(nn.Module):\n",
    "    def __init__(self, input_size=4, hidden_sizes=[8, 8], output_size=3):\n",
    "        \"\"\"\n",
    "        input_size: number of input features (e.g. 4 for Iris)\n",
    "        hidden_sizes: list of hidden layer sizes, e.g. [8, 8, 4]\n",
    "        output_size: number of output classes (e.g. 3 for Iris)\n",
    "        \"\"\"\n",
    "        super(ClassicalVariableModel_Relu, self).__init__()\n",
    "        \n",
    "        # Create a list of layer sizes\n",
    "        layer_sizes = [input_size] + hidden_sizes + [output_size]\n",
    "        \n",
    "        # Create a list of Linear layers\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(layer_sizes[i], layer_sizes[i+1])\n",
    "            for i in range(len(layer_sizes) - 1)\n",
    "        ])\n",
    "\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        # Go through all layers except the last one with ReLU\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = F.relu(layer(x))\n",
    "        # Apply softmax on the last layer (output)\n",
    "        x = F.softmax(self.layers[-1](x), dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe34eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class train_backprop():\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        iris = load_iris()\n",
    "        X = iris['data']\n",
    "        y = torch.tensor(iris['target'], dtype=torch.long)\n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = torch.tensor(scaler.fit_transform(X), dtype=torch.float)\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_scaled, y, test_size=0.2, random_state=2)  # Total n dat 150, so 120 for train and 30 for test\n",
    "\n",
    "        self.train_data = TensorDataset(X_train, y_train)\n",
    "        test_data = TensorDataset(X_test, y_test)\n",
    "\n",
    "        self.train_loader = DataLoader(self.train_data, shuffle=True, batch_size=12)  # batch size 12 so 10 batch\n",
    "        self.test_loader = DataLoader(test_data, batch_size=len(test_data.tensors[0]))  # batch size 30 max\n",
    "\n",
    "    def train_IRIS(self,\n",
    "                   model,\n",
    "                   epochs: int = 400) -> [np.array, np.array, np.array, np.array]:\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "        train_loss = np.zeros(epochs)\n",
    "        valid_loss = np.zeros(epochs)\n",
    "        train_accuracy = np.zeros(epochs)\n",
    "        valid_accuracy = np.zeros(epochs)\n",
    "\n",
    "        for epoch in tqdm.trange(epochs):\n",
    "            # Train\n",
    "            for X, y in self.train_loader:\n",
    "                y_pred = model(X)\n",
    "                loss = loss_fn(y_pred, y)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Test training data\n",
    "            with torch.no_grad():\n",
    "                X, y = self.train_data[:]\n",
    "                y_pred = model(X)\n",
    "                loss = loss_fn(y_pred, y)\n",
    "                train_loss[epoch] = loss.item()\n",
    "                y_pred_labels = torch.argmax(y_pred, axis=1)\n",
    "                train_accuracy[epoch] = torch.mean((y_pred_labels == y).float()).item()\n",
    "\n",
    "            # Validation\n",
    "            with torch.no_grad():\n",
    "                X, y = next(iter(self.test_loader))\n",
    "                y_pred = model(X)\n",
    "                loss = loss_fn(y_pred, y)\n",
    "                valid_loss[epoch] = loss.item()\n",
    "                y_pred_labels = torch.argmax(y_pred, axis=1)\n",
    "                valid_accuracy[epoch] = torch.mean((y_pred_labels == y).float()).item()\n",
    "\n",
    "        return train_loss, valid_loss, train_accuracy, valid_accuracy\n",
    "\n",
    "    def graph_train_IRIS(self, model, epochs: int = 400, n_runs: int = 15) -> None:\n",
    "        \"\"\"\n",
    "        Train the given model multiple times (n_runs) and save all results in a single CSV file.\n",
    "        The CSV includes model name, hidden layer configuration, and final metrics for each run.\n",
    "        Only the final run displays the training graph.\n",
    "        \"\"\"\n",
    "        import os\n",
    "        from datetime import datetime\n",
    "        import pandas as pd\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        os.makedirs(\"results\", exist_ok=True)\n",
    "\n",
    "        model_name = model.__class__.__name__\n",
    "\n",
    "        # Detect hidden layer sizes if possible\n",
    "        try:\n",
    "            hidden_sizes = [layer.out_features for layer in model.layers[:-1]]\n",
    "        except AttributeError:\n",
    "            hidden_sizes = [\"Unknown\"]\n",
    "\n",
    "        # Use timestamp of the first run for filename\n",
    "        global_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        all_runs = []  # store all runs here\n",
    "\n",
    "        for run in range(1, n_runs + 1):\n",
    "            model = model.__class__()  # re-instantiate model for each run\n",
    "            print(f\"üöÄ Run {run}/{n_runs} for {model_name} with layers {hidden_sizes}\")\n",
    "\n",
    "            train_loss, valid_loss, train_accuracy, valid_accuracy = self.train_IRIS(model, epochs)\n",
    "\n",
    "            final_train_loss = train_loss[-1]\n",
    "            final_valid_loss = valid_loss[-1]\n",
    "            final_train_acc = train_accuracy[-1]\n",
    "            final_valid_acc = valid_accuracy[-1]\n",
    "\n",
    "            all_runs.append({\n",
    "                \"timestamp\": global_timestamp,\n",
    "                \"model_name\": model_name,\n",
    "                \"hidden_sizes\": str(hidden_sizes),\n",
    "                \"epochs\": epochs,\n",
    "                \"run_number\": run,\n",
    "                \"final_train_loss\": final_train_loss,\n",
    "                \"final_valid_loss\": final_valid_loss,\n",
    "                \"final_train_accuracy\": final_train_acc,\n",
    "                \"final_valid_accuracy\": final_valid_acc\n",
    "            })\n",
    "\n",
    "            print(f\"üèÅ Validation accuracy (run {run}): {final_valid_acc:.3f}\")\n",
    "\n",
    "            # Show the training curves only for the last run\n",
    "            if run == n_runs:\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "                ax1.plot(train_accuracy, label=\"Train accuracy\")\n",
    "                ax1.plot(valid_accuracy, label=\"Validation accuracy\")\n",
    "                ax1.set_title(\"Accuracy\")\n",
    "                ax1.set_xlabel(\"epochs\")\n",
    "                ax1.set_ylim([0.0, 1.05])\n",
    "                ax1.legend()\n",
    "\n",
    "                ax2.plot(train_loss, label=\"Train loss\")\n",
    "                ax2.plot(valid_loss, label=\"Validation loss\")\n",
    "                ax2.set_title(\"Loss\")\n",
    "                ax2.set_xlabel(\"epochs\")\n",
    "                ax2.legend()\n",
    "                plt.show()\n",
    "\n",
    "        # Save all results after all runs\n",
    "        results_df = pd.DataFrame(all_runs)\n",
    "        csv_filename = f\"{model_name}_layers{hidden_sizes}_epochs{epochs}_{n_runs}runs_{global_timestamp}.csv\"\n",
    "        csv_path = os.path.join(\"results\", csv_filename)\n",
    "        results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "        print(f\"\\n‚úÖ All {n_runs} runs saved in one file ‚Üí {csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2acefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_backprop()\n",
    "# train_loss, valid_loss, train_accuracy, valid_accuracy = a.train_IRIS(Classic_Model())\n",
    "a.graph_train_IRIS(ClassicalVariableModel(hidden_sizes=[3,3]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
